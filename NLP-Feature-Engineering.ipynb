{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob  # for getting filepaths\n",
    "import os\n",
    "import re  # Import the re module\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset\n",
      "License(s): CC0-1.0\n",
      "Downloading hate-speech-and-offensive-language-dataset.zip to c:\\Users\\Bryan Chan\\Documents\\Projects\\HateSpeech_detection\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/1.01M [00:00<?, ?B/s]\n",
      " 99%|█████████▉| 1.00M/1.01M [00:00<00:00, 1.72MB/s]\n",
      "100%|██████████| 1.01M/1.01M [00:00<00:00, 1.70MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d mrmorj/hate-speech-and-offensive-language-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('hate-speech-and-offensive-language-dataset.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('hate_speech_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'hate_speech_dataset/labeled_data.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "df.head(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!\"@__BrighterDays: I can not just sit up ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!&amp;#8220;@selfiequeenbri: cause I'm tired of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" &amp;amp; you might not get ya bitch back &amp;amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" @rhythmixx_ :hobbies include: fighting Maria...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "5           5      3            1                   2        0      1   \n",
       "6           6      3            0                   3        0      1   \n",
       "7           7      3            0                   3        0      1   \n",
       "8           8      3            0                   3        0      1   \n",
       "9           9      3            1                   2        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
       "5  !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...  \n",
       "6  !!!!!!\"@__BrighterDays: I can not just sit up ...  \n",
       "7  !!!!&#8220;@selfiequeenbri: cause I'm tired of...  \n",
       "8  \" &amp; you might not get ya bitch back &amp; ...  \n",
       "9  \" @rhythmixx_ :hobbies include: fighting Maria...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the dataset\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new dataset from 'tweet' and 'class' column\n",
    "tweet_X = pd.DataFrame(df['tweet'])\n",
    "tweet_y = pd.DataFrame(df['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Text Preprocessing\n",
    "\n",
    "Lowercase the tweet, URLs, HTML entities, hashtags and mentions, punctuation, extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     rt as a woman you shouldnt complain about clea...\n",
       "1     rt boy dats coldtyga dwn bad for cuffin dat ho...\n",
       "2     rt dawg rt you ever fuck a bitch and she start...\n",
       "3                             rt she look like a tranny\n",
       "4     rt the shit you hear about me might be true or...\n",
       "5     the shit just blows meclaim you so faithful an...\n",
       "6     i can not just sit up and hate on another bitc...\n",
       "7     cause im tired of you big bitches coming for u...\n",
       "8            you might not get ya bitch back thats that\n",
       "9                 hobbies include fighting mariam bitch\n",
       "10    keeks is a bitch she curves everyone lol i wal...\n",
       "11                       murda gang bitch its gang land\n",
       "12           so hoes that smoke are losers yea go on ig\n",
       "13            bad bitches is the only thing that i like\n",
       "14                                  bitch get up off me\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def preprocess_tweet_typeB(tweet):\n",
    "    # Lowercase the tweet\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove HTML entities\n",
    "    tweet = re.sub(r'&\\w+;', '', tweet)\n",
    "    \n",
    "    # Remove hashtags and mentions\n",
    "    tweet = re.sub(r'#\\w+|@\\w+', '', tweet)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    tweet = tweet.strip()\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "# Apply the preprocessing function to the X_train dataset\n",
    "tweet_X_cleanedB = tweet_X['tweet'].apply(preprocess_tweet_typeB)\n",
    "\n",
    "# Display the first few cleaned tweet\n",
    "tweet_X_cleanedB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Text Preprocessing\n",
    "\n",
    "Additional Tokenization, remove Stopwords, and Lemmatization steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x0000020BB2DD9400>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Bryan Chan\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 790, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "                                                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Bryan Chan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1533, in enumerate\n",
      "    def enumerate():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatized_tokens)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Apply the preprocessing function to the tweet_X dataset\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m tweet_X_cleanedA \u001b[38;5;241m=\u001b[39m \u001b[43mtweet_X\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtweet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_tweet_typeA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Display the first few cleaned tweets\u001b[39;00m\n\u001b[0;32m     57\u001b[0m tweet_X_cleanedA\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m15\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Bryan Chan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4904\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4771\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4776\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4777\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4779\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4780\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4895\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4898\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4902\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bryan Chan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bryan Chan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Bryan Chan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bryan Chan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[11], line 45\u001b[0m, in \u001b[0;36mpreprocess_tweet_typeA\u001b[1;34m(tweet)\u001b[0m\n\u001b[0;32m     42\u001b[0m tokens \u001b[38;5;241m=\u001b[39m word_tokenize(tweet)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstop_words\u001b[49m]\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Lemmatize the tokens\u001b[39;00m\n\u001b[0;32m     48\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_tweet_typeA(tweet):\n",
    "    # Lowercase the tweet\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove HTML entities\n",
    "    tweet = re.sub(r'&\\w+;', '', tweet)\n",
    "    \n",
    "    # Remove hashtags and mentions\n",
    "    tweet = re.sub(r'#\\w+|@\\w+', '', tweet)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    tweet = tweet.strip()\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    \n",
    "    # Tokenize the tweet\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    \n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "# Apply the preprocessing function to the tweet_X dataset\n",
    "tweet_X_cleanedA = tweet_X['tweet'].apply(preprocess_tweet_typeA)\n",
    "\n",
    "# Display the first few cleaned tweets\n",
    "tweet_X_cleanedA.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kno PERSON\n",
      "rt js PERSON\n",
      "mufucka PERSON\n",
      "rt jsu PERSON\n",
      "omar johnson PERSON\n",
      "somethingod throwin PERSON\n",
      "ewww yuck PERSON\n",
      "shylock PERSON\n",
      "rt rt PERSON\n",
      "rt hun PERSON\n",
      "blanke beslissing PERSON\n",
      "rt oreos PERSON\n",
      "charlie PERSON\n",
      "george need PERSON\n",
      "rt dese PERSON\n",
      "nigga PERSON\n",
      "eatin nd beatin PERSON\n",
      "nah gurl PERSON\n",
      "ya PERSON\n",
      "ya pussy gon PERSON\n",
      "coulda PERSON\n",
      "ymas sws PERSON\n",
      "rt bitches tweeting PERSON\n",
      "rt jackie PERSON\n",
      "lmaoooo beiber PERSON\n",
      "kik PERSON\n",
      "rt bitches jus love PERSON\n",
      "rt trey songz PERSON\n",
      "rt bitches fwu PERSON\n",
      "rt rt word rt having zero PERSON\n",
      "rt jihadi PERSON\n",
      "rt yung berg PERSON\n",
      "rt hol PERSON\n",
      "rt uncle PERSON\n",
      "lebron PERSON\n",
      "rt hell yea PERSON\n",
      "lmfaoooooooooooooooooo PERSON\n",
      "rt ayy PERSON\n",
      "rt rt PERSON\n",
      "atl bvb sws PERSON\n",
      "fob PERSON\n",
      "charlie PERSON\n",
      "gail PERSON\n",
      "hey jim PERSON\n",
      "wana PERSON\n",
      "kevin hart PERSON\n",
      "mary jane PERSON\n",
      "lee castro PERSON\n",
      "anthony PERSON\n",
      "michael bay PERSON\n",
      "rt obama PERSON\n",
      "ali PERSON\n",
      "rt obama PERSON\n",
      "johnson PERSON\n",
      "lmao rt lmao rt PERSON\n",
      "rt joan rivers PERSON\n",
      "ron brownvin PERSON\n",
      "rt tom ford PERSON\n",
      "weet je PERSON\n",
      "al niet PERSON\n",
      "anthony davis PERSON\n",
      "bitchnun finna PERSON\n",
      "rt johnny cash PERSON\n",
      "mike PERSON\n",
      "chick flick PERSON\n",
      "charlie brown PERSON\n",
      "maco winnin PERSON\n",
      "dakari johnson PERSON\n",
      "rt bob marley PERSON\n",
      "ya dick PERSON\n",
      "fuckin PERSON\n",
      "rt hoodie PERSON\n",
      "mike PERSON\n",
      "omggggg nigga PERSON\n",
      "jerry black PERSON\n",
      "rt rt PERSON\n",
      "chris PERSON\n",
      "dana PERSON\n",
      "brenda PERSON\n",
      "rt lmao a PERSON\n",
      "teeny weeny PERSON\n",
      "rt fuck twerking PERSON\n",
      "rt andy PERSON\n",
      "earl PERSON\n",
      "ching chong ting o PERSON\n",
      "bri PERSON\n",
      "ohh yaa PERSON\n",
      "hoodrat PERSON\n",
      "homos heteros PERSON\n",
      "rt bitches PERSON\n",
      "rt son lmao rt PERSON\n",
      "putin PERSON\n",
      "kno wen PERSON\n",
      "chubby PERSON\n",
      "rt lightskin PERSON\n",
      "rt guy send PERSON\n",
      "rt fuck no PERSON\n",
      "juss PERSON\n",
      "keeps findin PERSON\n",
      "uncle tom PERSON\n",
      "rick ross PERSON\n",
      "rt blow PERSON\n",
      "rt smh nigga PERSON\n",
      "kno PERSON\n",
      "thincc PERSON\n",
      "rt yass PERSON\n",
      "goligoski PERSON\n",
      "ronny kamm PERSON\n",
      "billy goat PERSON\n",
      "rt cuddle PERSON\n",
      "swag PERSON\n",
      "lml nd PERSON\n",
      "uncle tom PERSON\n",
      "babe ruth PERSON\n",
      "dick bitch PERSON\n",
      "rt rihanna PERSON\n",
      "awww PERSON\n",
      "trap house PERSON\n",
      "rt mrs harden PERSON\n",
      "rt bitches twerk PERSON\n",
      "chill zahra PERSON\n",
      "rt twitch plays PERSON\n",
      "nomo PERSON\n",
      "george zimmerman PERSON\n",
      "rt yo PERSON\n",
      "rt pussy ass nigga PERSON\n",
      "rt karmas PERSON\n",
      "maxi dweeb PERSON\n",
      "valerie jarrett PERSON\n",
      "mcflurry PERSON\n",
      "charlie brown PERSON\n",
      "rt king PERSON\n",
      "graham cracker cereal PERSON\n",
      "tony parker PERSON\n",
      "rt lame bitches PERSON\n",
      "niggaz PERSON\n",
      "dick PERSON\n",
      "michonne PERSON\n",
      "austin PERSON\n",
      "hidey hidey PERSON\n",
      "duo etamcru PERSON\n",
      "lmao angela PERSON\n",
      "masahiro tanaka PERSON\n",
      "rt michael PERSON\n",
      "jimmy PERSON\n",
      "chris PERSON\n",
      "rt curious george the curious PERSON\n",
      "jennifer hudson PERSON\n",
      "charlie PERSON\n",
      "bullshittin bevo PERSON\n",
      "omfg rt PERSON\n",
      "nigga yu PERSON\n",
      "anderson PERSON\n",
      "lmaoooooo lil dave PERSON\n",
      "intvw PERSON\n",
      "mitt romney PERSON\n",
      "jim hogg PERSON\n",
      "jhud PERSON\n",
      "rt rt PERSON\n",
      "joe PERSON\n",
      "nigga PERSON\n",
      "rt kim kardashian PERSON\n",
      "joe PERSON\n",
      "rt downloaded flappy bird PERSON\n",
      "rt am PERSON\n",
      "hank jr PERSON\n",
      "honkie PERSON\n",
      "nigga fuck PERSON\n",
      "callin PERSON\n",
      "rt bullying PERSON\n",
      "jose soto PERSON\n",
      "kno PERSON\n",
      "kno PERSON\n",
      "rt jessica PERSON\n",
      "paula dean PERSON\n",
      "nigga jesus PERSON\n",
      "emoji PERSON\n",
      "charlie ericksen PERSON\n",
      "edgar bautista PERSON\n",
      "austin PERSON\n",
      "swag witch PERSON\n",
      "nigga PERSON\n",
      "rt kmm PERSON\n",
      "watchu mad foe PERSON\n",
      "rt guy PERSON\n",
      "dick anymore PERSON\n",
      "rt jacob PERSON\n",
      "wallace PERSON\n",
      "ily2 PERSON\n",
      "rt nah PERSON\n",
      "tweety bird tho PERSON\n",
      "hahamonkey feeti PERSON\n",
      "trojan PERSON\n",
      "nig lolsam PERSON\n",
      "lmfaoooo rt PERSON\n",
      "lovin PERSON\n",
      "charlie murphys PERSON\n",
      "jammin PERSON\n",
      "brie PERSON\n",
      "rt rt PERSON\n",
      "babbyyy PERSON\n",
      "wit yo PERSON\n",
      "wendy davis PERSON\n",
      "keezy ooooow PERSON\n",
      "tightey whitey whitey jr taylor PERSON\n",
      "lmaooo nigga PERSON\n",
      "lmaoooooo rt PERSON\n",
      "yea mike PERSON\n",
      "muthafucka PERSON\n",
      "bryan PERSON\n",
      "rt gwen PERSON\n",
      "barrys whitey brownshirts PERSON\n",
      "rt fuck half PERSON\n",
      "kraut PERSON\n",
      "daniel pearl PERSON\n",
      "rt bro forreal PERSON\n",
      "charlie manson PERSON\n",
      "lmfaooo PERSON\n",
      "rt charlie PERSON\n",
      "rt miley PERSON\n",
      "rt sooo PERSON\n",
      "kno PERSON\n",
      "rt kill whitey PERSON\n",
      "rt kim PERSON\n",
      "rt finna make PERSON\n",
      "rt jimmy PERSON\n",
      "jesus PERSON\n",
      "martin mccann PERSON\n",
      "rt white PERSON\n",
      "abby PERSON\n",
      "rt rip robin williams PERSON\n",
      "kno PERSON\n",
      "rt bum ass PERSON\n",
      "hahahhahaa rt PERSON\n",
      "rt jeter PERSON\n",
      "rt yankees PERSON\n",
      "rafael depaula PERSON\n",
      "manchester terror PERSON\n",
      "flappy bird PERSON\n",
      "rt keshun PERSON\n",
      "krueger mask PERSON\n",
      "yas bitches yas PERSON\n",
      "rt fuck shes PERSON\n",
      "lil wayne PERSON\n",
      "lmsoooo dawg PERSON\n",
      "rt karma PERSON\n",
      "hussein PERSON\n",
      "tim tom PERSON\n",
      "kinda rt PERSON\n",
      "jameis winston PERSON\n",
      "rt rt PERSON\n",
      "rt rt last PERSON\n",
      "horny milf PERSON\n",
      "god PERSON\n",
      "tony blair PERSON\n",
      "rt flappy bird PERSON\n",
      "nah lemme PERSON\n",
      "rt rt PERSON\n",
      "rt benzino bitch PERSON\n",
      "dis nicca PERSON\n",
      "bruhhhh PERSON\n",
      "jesus murphy PERSON\n",
      "okc nigguh PERSON\n",
      "george rockwell PERSON\n",
      "ichiro rf roberts PERSON\n",
      "antonio stfu PERSON\n",
      "rt btw hillary PERSON\n",
      "kanye PERSON\n",
      "rt rudy PERSON\n",
      "rt fuck islam PERSON\n",
      "bobby PERSON\n",
      "nigga PERSON\n",
      "johnny depps PERSON\n",
      "rt pussy PERSON\n",
      "jeezy PERSON\n",
      "rt fuuck jalens PERSON\n",
      "rt sav PERSON\n",
      "jack PERSON\n",
      "lilith lee PERSON\n",
      "james x PERSON\n",
      "rt fuck twerking PERSON\n",
      "nigga PERSON\n",
      "winston niccas lmao PERSON\n",
      "rt twitter PERSON\n",
      "frontin PERSON\n",
      "harris PERSON\n",
      "nig nog PERSON\n",
      "jakes bitch PERSON\n",
      "williams PERSON\n",
      "dick beetle PERSON\n",
      "nah PERSON\n",
      "kim PERSON\n",
      "nigga pee wee PERSON\n",
      "jealous bitches PERSON\n",
      "rt sheryl PERSON\n",
      "john PERSON\n",
      "talkin brokanese PERSON\n",
      "lmaoo rt stfuwho are PERSON\n",
      "katiedodds15 PERSON\n",
      "rt cincy PERSON\n",
      "chuck PERSON\n",
      "kendrick PERSON\n",
      "jim crow PERSON\n",
      "steve calvin annis cunt PERSON\n",
      "rt fuck the PERSON\n",
      "rt yo PERSON\n",
      "niggas actin PERSON\n",
      "rt lil bitch PERSON\n",
      "johnny manziel PERSON\n",
      "rt muhammad ali roy gracie PERSON\n",
      "graham crackers PERSON\n",
      "paul george dick PERSON\n",
      "ahaha pussy PERSON\n",
      "nigga PERSON\n",
      "rt rip tom foley rip greg sikorskey rip charlie anaya PERSON\n",
      "nigga PERSON\n",
      "rt girlfriends PERSON\n",
      "sam PERSON\n",
      "uncle tom PERSON\n",
      "ngger PERSON\n",
      "god PERSON\n",
      "joe budden PERSON\n",
      "nigga PERSON\n",
      "dr gero PERSON\n",
      "das sum PERSON\n",
      "lmaoooo rt lol ok pal PERSON\n",
      "momma PERSON\n",
      "ray allen PERSON\n",
      "dis nicca lame PERSON\n",
      "lmfaoooo PERSON\n",
      "az PERSON\n",
      "ciara PERSON\n",
      "rt kevin PERSON\n",
      "larry PERSON\n",
      "deandre jordan PERSON\n",
      "rt yankees PERSON\n",
      "rt iggy azalea PERSON\n",
      "fuckin PERSON\n",
      "rt pussy PERSON\n",
      "jamie PERSON\n",
      "bobby bitch PERSON\n",
      "god PERSON\n",
      "elizabeth PERSON\n",
      "naw nigga PERSON\n",
      "chick flick PERSON\n",
      "ma PERSON\n",
      "rt iont PERSON\n",
      "muthafucka PERSON\n",
      "brady PERSON\n",
      "jayz PERSON\n",
      "rt yo bitch PERSON\n",
      "maas PERSON\n",
      "sharkette pussy PERSON\n",
      "screamin fuck kd PERSON\n",
      "wayne PERSON\n",
      "marcus smart PERSON\n",
      "yo fuck the PERSON\n",
      "teanna trump PERSON\n",
      "rt ya bitches luv PERSON\n",
      "rt spiderman PERSON\n",
      "lmfaoooo swag bitch PERSON\n",
      "nigga PERSON\n",
      "jack PERSON\n",
      "okaaaaay bitch PERSON\n",
      "dick cry PERSON\n",
      "lol lebron PERSON\n",
      "charlie morton PERSON\n",
      "rt mertesacker saying PERSON\n",
      "morele vinger PERSON\n",
      "kirk franklin PERSON\n",
      "rt abel PERSON\n",
      "fat bitches PERSON\n",
      "kanye PERSON\n",
      "ya fuckin PERSON\n",
      "rt oomf PERSON\n",
      "damn bro PERSON\n",
      "thomas kienzle PERSON\n",
      "brandon jennings PERSON\n",
      "coolin PERSON\n",
      "god PERSON\n",
      "harry PERSON\n",
      "rt hunter PERSON\n",
      "rt fuck PERSON\n",
      "eff dem PERSON\n",
      "gerry PERSON\n",
      "nah PERSON\n",
      "beth PERSON\n",
      "baker PERSON\n",
      "jesus soto karass PERSON\n",
      "rt lmfaoooooooooo trash PERSON\n",
      "da msnbc PERSON\n",
      "kmichelle PERSON\n",
      "rt lmaoooooo rt nymphos PERSON\n",
      "lmaooo coley PERSON\n",
      "charlie parr PERSON\n",
      "trotin thotin PERSON\n",
      "charlie daniels PERSON\n",
      "kim PERSON\n",
      "rt fuck these lancaster PERSON\n",
      "ya kno nigga PERSON\n",
      "albino chick PERSON\n",
      "charlie rangel PERSON\n",
      "rt fuck twerking PERSON\n",
      "swag PERSON\n",
      "juss askd yu PERSON\n",
      "rt bitches brag PERSON\n",
      "patty PERSON\n",
      "nigga PERSON\n",
      "god PERSON\n",
      "chris brown song PERSON\n",
      "rt bitches PERSON\n",
      "stacks kendrick PERSON\n",
      "kanye PERSON\n",
      "joe cortez PERSON\n",
      "erica PERSON\n",
      "jerkin yo PERSON\n",
      "dick ya PERSON\n",
      "rt dese PERSON\n",
      "nigga PERSON\n",
      "mickey mouse PERSON\n",
      "rt yo mama PERSON\n",
      "rt tell PERSON\n",
      "rt yg PERSON\n",
      "charlie brown PERSON\n",
      "rt nigga fuck PERSON\n",
      "kinda PERSON\n",
      "chatt niccas PERSON\n",
      "charlie xcx PERSON\n",
      "duh aka PERSON\n",
      "bobby bitchby PERSON\n",
      "rt suicide PERSON\n",
      "kim k PERSON\n",
      "rt don lemon PERSON\n",
      "chubby PERSON\n",
      "kinda PERSON\n",
      "damn bitches PERSON\n",
      "actin lame PERSON\n",
      "obispo ho PERSON\n",
      "rt karate bitch PERSON\n",
      "charlie baker PERSON\n",
      "tom menino PERSON\n",
      "rt nigga PERSON\n",
      "ight nigga PERSON\n",
      "george make PERSON\n",
      "rt bitches PERSON\n",
      "charlie PERSON\n",
      "blatt PERSON\n",
      "rt nigga PERSON\n",
      "maria ozawa PERSON\n",
      "brandy wanna PERSON\n",
      "sosa PERSON\n",
      "sammy sosa PERSON\n",
      "charlie st cloud PERSON\n",
      "marijuana PERSON\n",
      "lmaoooo rt PERSON\n",
      "al green PERSON\n",
      "psh PERSON\n",
      "rt icepick punk PERSON\n",
      "brown PERSON\n",
      "meaty PERSON\n",
      "charlie PERSON\n",
      "lmfaooooooooo bro rt PERSON\n",
      "ray rice PERSON\n",
      "rt dick PERSON\n",
      "rt ray j PERSON\n",
      "whoooo lemme PERSON\n",
      "ben franklin PERSON\n",
      "alex ramos PERSON\n",
      "xxxmas PERSON\n",
      "mike vick PERSON\n",
      "luke PERSON\n",
      "rt neta bullshit PERSON\n",
      "rt yeaaaaahhhh like PERSON\n",
      "john kerry PERSON\n",
      "ho PERSON\n",
      "wayne brady PERSON\n",
      "jesus PERSON\n",
      "alex fuck PERSON\n",
      "rt fellas guess PERSON\n",
      "rt yo this PERSON\n",
      "wallah ill drop kick this bitch PERSON\n",
      "rt frfr come PERSON\n",
      "rt twitter PERSON\n",
      "lmaoo PERSON\n",
      "tiene cansada PERSON\n",
      "ya pussy tho k PERSON\n",
      "masahiro tanaka PERSON\n",
      "rt horrible rt invader PERSON\n",
      "rt johnny manziels PERSON\n",
      "roberts PERSON\n",
      "rt fuck a PERSON\n",
      "kanye west PERSON\n",
      "johnny PERSON\n",
      "chuck PERSON\n",
      "dumpster rt gahbage rt PERSON\n",
      "dana PERSON\n",
      "rt reds fan snags foul ball PERSON\n",
      "rt bitches PERSON\n",
      "charlie PERSON\n",
      "lee PERSON\n",
      "swag PERSON\n",
      "nicki gottem PERSON\n",
      "nicki PERSON\n",
      "graham crackers PERSON\n",
      "maya angelo PERSON\n",
      "rt keebler elves gone PERSON\n",
      "rt bro fuck PERSON\n",
      "tryn PERSON\n",
      "yu nigga PERSON\n",
      "trojan PERSON\n",
      "rt white twitter PERSON\n",
      "rt rt PERSON\n",
      "charlie brown PERSON\n",
      "sarah mocks PERSON\n",
      "rt bitches PERSON\n",
      "bob cuh PERSON\n",
      "ho PERSON\n",
      "niccas catchin dem PERSON\n",
      "text ya bitch PERSON\n",
      "rt needy PERSON\n",
      "mimi PERSON\n",
      "gov rick scott kicks PERSON\n",
      "charlie crists ass PERSON\n",
      "jennifer mccarthy PERSON\n",
      "melisa erwin PERSON\n",
      "momma lmfaoooo PERSON\n",
      "lmao shorty twitter PERSON\n",
      "cmon PERSON\n",
      "sam PERSON\n",
      "rt rt ike finna PERSON\n",
      "dick whipped PERSON\n",
      "rt rt carrot cake PERSON\n",
      "callin PERSON\n",
      "ya beard PERSON\n",
      "ya PERSON\n",
      "rt rt PERSON\n",
      "inda PERSON\n",
      "yo bitch wanna PERSON\n",
      "michael mcdonald PERSON\n",
      "charlie rangel PERSON\n",
      "rt tennessee PERSON\n",
      "maddie karissa PERSON\n",
      "cierra PERSON\n",
      "keke PERSON\n",
      "rt sasuke PERSON\n",
      "ho lmao PERSON\n",
      "rt fuck michigan PERSON\n",
      "rt lmaooo PERSON\n",
      "flappy bird PERSON\n",
      "jessie andrews PERSON\n",
      "boy yung thug PERSON\n",
      "evan williams PERSON\n",
      "rt bitches talking PERSON\n",
      "rt rt rt PERSON\n",
      "alex smith PERSON\n",
      "ayy carlton PERSON\n",
      "boy nah PERSON\n",
      "pre1800s PERSON\n",
      "joey PERSON\n",
      "jackie peyton PERSON\n",
      "woof woof PERSON\n",
      "rt karrenchi PERSON\n",
      "aerin PERSON\n",
      "robert frost PERSON\n",
      "albino mexican PERSON\n",
      "fuckin fine tho dam PERSON\n",
      "fucken pussy PERSON\n",
      "rt lmao PERSON\n",
      "rt bet PERSON\n",
      "lmaoo PERSON\n",
      "rt imma PERSON\n",
      "miguel PERSON\n",
      "rt nigga PERSON\n",
      "rt free pancake PERSON\n",
      "kinda PERSON\n",
      "rt killin PERSON\n",
      "woof woof PERSON\n",
      "nigga lmao PERSON\n",
      "matt PERSON\n",
      "luvin uncle PERSON\n",
      "wana PERSON\n",
      "rt lmao bruh PERSON\n",
      "rt ncs PERSON\n",
      "rt ibs irritable PERSON\n",
      "rt ltimahora el republicano PERSON\n",
      "charlie baker PERSON\n",
      "rt ya PERSON\n",
      "kim PERSON\n",
      "fuckin PERSON\n",
      "nigga PERSON\n",
      "rt yankees PERSON\n",
      "rt hannah PERSON\n",
      "rt flappy birds PERSON\n",
      "rt messi PERSON\n",
      "imma catch ebola PERSON\n",
      "kno PERSON\n",
      "rosa mexicano PERSON\n",
      "anne frank PERSON\n",
      "ymas fob PERSON\n",
      "rt bitches lover PERSON\n",
      "graham PERSON\n",
      "limewire sooo PERSON\n",
      "waka PERSON\n",
      "rt kobe PERSON\n",
      "killt dat PERSON\n",
      "kevin hart PERSON\n",
      "rt lmao dis bitch PERSON\n",
      "kno PERSON\n",
      "ova bitches PERSON\n",
      "ya momma PERSON\n",
      "rack city PERSON\n",
      "alex smith PERSON\n",
      "charlotte goon niccas gon PERSON\n",
      "rt yankees PERSON\n",
      "livin PERSON\n",
      "jtfooooo PERSON\n",
      "lmao rt PERSON\n",
      "michael clifford role PERSON\n",
      "rt bruh PERSON\n",
      "rt jihadi PERSON\n",
      "nigga PERSON\n",
      "rt ya bitches PERSON\n",
      "rt instagram PERSON\n",
      "jaillmao PERSON\n",
      "jim crow PERSON\n",
      "rt nooo PERSON\n",
      "rt shut PERSON\n",
      "lamar PERSON\n",
      "jenna PERSON\n",
      "john grisham PERSON\n",
      "rt mook gon wi 30 PERSON\n",
      "ya PERSON\n",
      "rt rip PERSON\n",
      "rt fam rt PERSON\n",
      "vincent van gogh PERSON\n",
      "rt mary PERSON\n",
      "nigga PERSON\n",
      "ya PERSON\n",
      "kno PERSON\n",
      "rt money clothes PERSON\n",
      "rt flappy bird PERSON\n",
      "charlie sheens PERSON\n",
      "lmao rt lowkey peyton PERSON\n",
      "coulda PERSON\n",
      "kim k PERSON\n",
      "eatin grin PERSON\n",
      "rt eatin the pussy PERSON\n",
      "ballin PERSON\n",
      "kno niccas PERSON\n",
      "fuckin PERSON\n",
      "nah son PERSON\n",
      "james shy PERSON\n",
      "dave chappelle voice PERSON\n",
      "nigga PERSON\n"
     ]
    }
   ],
   "source": [
    "# take a 20% sample of the 'tweet_X_cleanedB' data\n",
    "tweet_sample = tweet_X_cleanedB.sample(frac=0.2, random_state=42)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#let's get the named entities:\n",
    "doc = [nlp(sentence) for sentence in tweet_sample]\n",
    "for i in doc:\n",
    "    for ent in i.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            print(ent.text, ent.label_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Preprocessing\n",
    "XA_train, XA_test, yA_train, yA_test = train_test_split(tweet_X_cleanedA, tweet_y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=tweet_y,\n",
    "                                                    random_state=42)\n",
    "print(XA_train.head(5))\n",
    "print(XA_test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Preprocessing\n",
    "XB_train, XB_test, yB_train, yB_test = train_test_split(tweet_X_cleanedB, tweet_y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=tweet_y,\n",
    "                                                    random_state=42)\n",
    "print(XB_train.head(5))\n",
    "print(XB_test.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Supervised Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Models and Metrics\n",
    "from sklearn.naive_bayes import MultinomialNB        # Multinomial Naive Bayes model classifier model\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression model classifier\n",
    "from sklearn.ensemble import RandomForestClassifier  # Random Forest model classifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultinomialNB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bccuracy : 0.8575751462578173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.02      0.04       286\n",
      "           1       0.86      0.98      0.92      3838\n",
      "           2       0.85      0.57      0.68       833\n",
      "\n",
      "    accuracy                           0.86      4957\n",
      "   macro avg       0.68      0.52      0.55      4957\n",
      "weighted avg       0.83      0.86      0.83      4957\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bryan Chan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# simple preprocessing\n",
    "vectorizer = CountVectorizer() # Convert text data to numerical features\n",
    "XB_train_vect = vectorizer.fit_transform(XB_train)\n",
    "XB_test_vect = vectorizer.transform(XB_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(XB_train_vect, yB_train) # Fit model to the Training Data\n",
    "\n",
    "y_pred = model.predict(XB_test_vect) \n",
    "\n",
    "print(f\"Bccuracy : {accuracy_score(yB_test, y_pred)}\")\n",
    "print(classification_report(yB_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8616098446641114\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.03      0.06       286\n",
      "           1       0.87      0.98      0.92      3838\n",
      "           2       0.86      0.59      0.70       833\n",
      "\n",
      "    accuracy                           0.86      4957\n",
      "   macro avg       0.69      0.54      0.56      4957\n",
      "weighted avg       0.83      0.86      0.83      4957\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bryan Chan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Advanced Preprocessing\n",
    "vectorizer = CountVectorizer() # Convert text data to numerical features\n",
    "XA_train_vect = vectorizer.fit_transform(XA_train)\n",
    "XA_test_vect = vectorizer.transform(XA_test)\n",
    "\n",
    "model = MultinomialNB() # Initialize the MultinomialNB model without class_weight\n",
    "model.fit(XA_train_vect, yA_train) # Fit model to the Training Data\n",
    "\n",
    "y_pred = model.predict(XA_test_vect) \n",
    "\n",
    "print(f\"Accuracy : {accuracy_score(yA_test, y_pred)}\")\n",
    "print(classification_report(yA_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bryan Chan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\Bryan Chan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8876336493847085\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.47      0.44       286\n",
      "           1       0.95      0.91      0.93      3838\n",
      "           2       0.80      0.93      0.86       833\n",
      "\n",
      "    accuracy                           0.89      4957\n",
      "   macro avg       0.72      0.77      0.74      4957\n",
      "weighted avg       0.90      0.89      0.89      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple Preprocessing\n",
    "vectorizer = CountVectorizer() # Convert text data to numerical features\n",
    "XB_train_vect = vectorizer.fit_transform(XB_train)\n",
    "XB_test_vect = vectorizer.transform(XB_test)\n",
    "\n",
    "model = LogisticRegression(multi_class='ovr', solver='liblinear', class_weight='balanced')\n",
    "model.fit(XB_train_vect, yB_train) # Fit model to the Training Data\n",
    "\n",
    "y_pred = model.predict(XB_test_vect) \n",
    "\n",
    "print(f\"Accuracy : {accuracy_score(yB_test, y_pred)}\")\n",
    "print(classification_report(yB_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8862215049425055\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.44      0.44       286\n",
      "           1       0.95      0.91      0.93      3838\n",
      "           2       0.78      0.92      0.84       833\n",
      "\n",
      "    accuracy                           0.89      4957\n",
      "   macro avg       0.72      0.76      0.74      4957\n",
      "weighted avg       0.89      0.89      0.89      4957\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bryan Chan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1310: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\Bryan Chan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Advanced Preprocessing\n",
    "vectorizer = CountVectorizer() # Convert text data to numerical features\n",
    "XA_train_vect = vectorizer.fit_transform(XA_train)\n",
    "XA_test_vect = vectorizer.transform(XA_test)\n",
    "\n",
    "model = LogisticRegression(multi_class='ovr', solver='liblinear', class_weight='balanced')\n",
    "model.fit(XA_train_vect, yA_train) # Fit model to the Training Data\n",
    "\n",
    "y_pred = model.predict(XA_test_vect) \n",
    "\n",
    "print(f\"Accuracy : {accuracy_score(yA_test, y_pred)}\")\n",
    "print(classification_report(yA_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bryan Chan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8628202541859996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.14      0.20       286\n",
      "           1       0.87      0.97      0.92      3838\n",
      "           2       0.88      0.61      0.72       833\n",
      "\n",
      "    accuracy                           0.86      4957\n",
      "   macro avg       0.70      0.57      0.61      4957\n",
      "weighted avg       0.84      0.86      0.84      4957\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "vectorizer = CountVectorizer() # Convert text data to numerical features\n",
    "XB_train_vect = vectorizer.fit_transform(XB_train)\n",
    "XB_test_vect = vectorizer.transform(XB_test)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, \n",
    "                               random_state=42, \n",
    "                               class_weight='balanced')\n",
    "model.fit(XB_train_vect, yB_train) # Fit model to the Training Data\n",
    "\n",
    "y_pred = model.predict(XB_test_vect) \n",
    "\n",
    "print(f\"Accuracy : {accuracy_score(yB_test, y_pred)}\")\n",
    "print(classification_report(yB_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesis_iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
